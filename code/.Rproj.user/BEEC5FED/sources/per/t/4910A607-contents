---
title: "Analysis_Manoj"
output: html_notebook
---




```{r}
#import the libraries we need
library(jsonlite)
library(glue)
library(readr)
library(lmtest) 
library(forecast)
library(DIMORA)
library(ggplot2)
library(quantmod)
library(TTR) # Technical Trading Rules
```


```{r}
btc <- read_csv("M://Crypto_Analysis/BTC-USD.csv", show_col_types = FALSE)
eth <- read_csv("M://Crypto_Analysis/ETH-USD.csv", show_col_types = FALSE)
btc1 <- read_csv("M://Crypto_Analysis/BTC-USD-1.csv", show_col_types = FALSE)
```


```{r}
btc
eth

```
```{r}
btc
eth
```
Since, Five days corresponds to one week of trading days, 10 days corresponds to two weeks of trading days, and 20 days corresponds to approximately one month of
trading days we have removed Saturday and Sunday where most trades does not happen.

Relative Strength Index Indicator <- It is a momentum indicator that measures the magnitude of recent price changes to analyze overbought or oversold conditions.

Readings below 30 generally indicate that the stock is oversold, while readings above 70 indicate that it is overbought.Generally, 14 days are use as default for computation purposes.

```{r}
for_date <- function(x){
  return(as.Date(as.POSIXct(strptime(x,"%d/%m/%Y"))))
}
```


```{r}
mod_date= lapply(btc[2:1796,1],for_date)
```

```{r}
btc.xts <- xts(btc[1:1795,2:8],order.by=mod_date$Date)

```


```{r}
btc.xts <- subset(btc.xts, btc.xts$Weekday!=6 & btc.xts$Weekday!=7)

#man.xts <- subset(btc.xts$Adjusted, man.xts$Weekday!=6 & btc.xts$Weekday!=7)

#n.xts <- man.xts[,2]
```

```{r}
btc <- subset(btc, btc$Weekday!=6 & btc$Weekday!=7)
eth <- subset(eth, eth$Day!="Sat" & eth$Day!="Sun")
```


```{r}
rsi <- RSI(Cl(btc.xts),n=14)
barChart(btc.xts,theme=chartTheme("white"))
addRSI(n=14)
```
stochastic oscillator <- The stochastic oscillator is a momentum indicator that relates the location of each day's close relative to the high/low range over the past 14 days.

nFastK <- number of past periods to use(14 is chosen)
nFastD <- number smoothing periods to apply to fastK.
nSlowD <-  the number smoothing periods to apply to fastD.

```{r}
stochOSC <-stoch(btc.xts[,c("High","Low","Close")],nFastK = 14,
  nFastD = 3,
  nSlowD = 3,)
StoFASTK = stochOSC$fastK
StoFASTD = stochOSC$fastD
StoSLOWK = stochOSC$slowD
tail(stochOSC,10)
```

```{r}

plot(tail(stochOSC[,"fastK"], 100), type="l",
    main="Fast %K ", ylab="")
```
The average directional index (ADX) is used to determine when the price is trending strongly.Some people only use this indicator to initiate Buy or Sell.

If AdX values is in range of:

0-25	Absent or Weak Trend
25-50	Strong Trend
50-75	Very Strong Trend
75-100	Extremely Strong Trend
```{r}
adx <- ADX(btc.xts[,c("High","Low","Close")])$ADX
tail(adx,10)

```
```{r}
plot(tail(adx, 100), type="l",
    main="Average Directional Index ", ylab="")
```

Price rate of change is calculated by subtracting the old value from the current value of the variable and dividing it by the old value. It is expressed as a percentage.

```{r}
roc <- ROC(btc.xts[,"Close"])
tail(roc,n=3)
```
On-balance volume (OBV) is a technical indicator of momentum, using volume changes to make price predictions.

```{r}
obv <- OBV(btc.xts[,"Close"], btc.xts[,"Volume"])
tail(obv,n=3)
```

The Money Flow Index (MFI) is a technical oscillator that uses price and volume data for identifying overbought or oversold signals in an asset
```{r}
mfi <- MFI(btc.xts[,"Close"], btc.xts[,"Volume"], n =14)
tail(mfi,n=3)
```
The Williams Accumulation / Distribution (AD) line is a measure of market momentum.


Distribution of the security is indicated when the security is making a new high and the A/D indicator is failing to make a new high. Sell.

Accumulation of the security is indicated when the security is making a new low and the A/D indicator is failing to make a new low. Buy.

```{r}
ad <- williamsAD(btc.xts[,c("High","Low","Close")])
tail(ad,n=3)
```
Moving averages are used by investors and traders to track and identify trends by smoothing normal day-to-day price fluctuations.

For example 50 day Moving average calculates average price of BTC over past 50 days which can be used for support, resistance level indicator.
```{r}
MA50 <-SMA(Cl(btc.xts),n=50)
MA200 <-SMA(Cl(btc.xts),n=200)
tail(MA50,n=3)
tail(MA200,n=3)

```

The MACD(moving average convergence/divergence) line is calculated by subtracting the 14-period EMA from the 3-period EMA. 

An EMA is a type of moving average (MA) that places a greater weight and significance on the most recent data points.

```{r}
macd <- MACD(Cl(btc.xts), nFast=14, nSlow=3,
             nSig=9, percent=FALSE)
MACD <- macd$macd
MACDSignal <- macd$signal
tail(macd,n=5)
```

```{r}
plot(tail(MACD, 100), type="l",
    main="MACD AND MOVING AVGS.", ylab="")
lines(MACDSignal, lwd=2, col=2)
lines(MA50, lwd=2, col=3)
lines(MA200, lwd=2, col=4)
```
Macro-Economic Variables:

Economic policy uncertainty(EPU) is measured using the US economic
policy uncertainty index and economic market uncertainty is measured using the economic market uncertainty index(EMU). 
```{r}
EPU <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/EPU.csv",show_col_types = FALSE)
EMU <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/EMU.csv",show_col_types = FALSE)
```

We get  values of 5 working days in week
```{r}
EPU <- subset(EPU, EPU$Day!="Sat" & EPU$Day!="Sun")$USEPUINDXD
EMU <- subset(EMU, EMU$Day!="Sat" & EMU$Day!="Sun")$WLEMUINDXD
```
Check null and na values if wxist in EPU and EMU    
```{r}
is.null(EPU)
is.null(EMU)
sum(is.na(EPU))
sum(is.na(EMU))
```

Interest rates are measured using the yield on the US ten-year bond (Tenyrbond), three-month T-bill (ThreemTbill), and the spread (spread) between these variables.
```{r}
Tenyrbond <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/Tenyrbond.csv",show_col_types = FALSE)$THREEFY10
ThreemTbill <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/ThreemTbill.csv",show_col_types = FALSE)$DTB3
```
In this dataset the Sat and Sun are already removed therefore check of na and null values need to be done.

```{r}
is.null(Tenyrbond)
is.null(ThreemTbill)
sum(is.na(Tenyrbond))
sum(is.na(ThreemTbill))
```

We can see there are coloumns which contains just "." instead of null or na so we need to replace 
```{r}
Tenyrbond['.' == Tenyrbond] = round(mean(as.numeric(Tenyrbond[-which('.' == Tenyrbond)])),4)
ThreemTbill['.' == ThreemTbill] = round(mean(as.numeric(ThreemTbill[-which('.' == ThreemTbill)])),4)
```


Equity and oil market uncertainty is measured using the VIX and the OVX respectively. The VIX is the CBOE volatility index that represents the US stock market expectations of volatility for the next 30 days. OVX is an estimate of the expected volatility over the next thirty days of the US crude oil prices.

```{r}
OVX <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/OVX.csv",show_col_types = FALSE)$OVX
VIX <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/VIX.csv",show_col_types = FALSE)$VIXCLS
```
In this dataset the Sat and Sun are already removed therefore check of na and null values need to be done.

```{r}
is.null(OVX)
is.null(VIX)
sum(is.na(OVX))
sum(is.na(VIX))
```

We can see there are columns which contains just "." instead of null or na so we need to replace with mean. 
```{r}
OVX['.' == OVX] = round(mean(as.numeric(OVX[-which('.' == OVX)])),2)
VIX['.' == VIX] = round(mean(as.numeric(VIX[-which('.' == VIX)])),2)
```


Variables are included for expected five-year inflation (Fiveyrinfexp) and break-even inflation (be_inflation). The equity market volatility infectious disease tracker (EMV_IDT) is included to account for the impact of the COVID-19 pandemic.

```{r}
Fiveyrinfexp <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/Fiveyrinfexp.csv",show_col_types = FALSE)
dat=Fiveyrinfexp$Date
Fiveyrinfexp <- Fiveyrinfexp$T5YIE
be_inflation <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/be_inflation.csv",show_col_types = FALSE)$T10YIE
EMV_IDT <- read_csv("C://Users/manoj/Desktop/BEFD_Project_Images/EMV_IDT.csv",show_col_types = FALSE)
```
In the Fiveyrinfexp & be_inflation datasets the Sat and Sun are already removed therefore check of na and null values need to be done but for EMV_IDT has sat and sumday therefore the subset must be considered for that dataset.

```{r}
EMV_IDT <- subset(EMV_IDT, EMV_IDT$Day!="Sat" & EMV_IDT$Day!="Sun")$INFECTDISEMVTRACKD
is.null(Fiveyrinfexp)
is.null(be_inflation)
is.null(EMV_IDT)
sum(is.na(Fiveyrinfexp))
sum(is.na(be_inflation))
sum(is.na(EMV_IDT)) 
```
```{r}
which(is.na(EMV_IDT))

EMV_IDT[is.na(EMV_IDT)] <- mean(EMV_IDT, na.rm = TRUE)


```
```{r}

```

We can see there are columns which contains just "." instead of null or na so we need to replace with mean. and col 1263 in EMV_IDT also has same "." in place of number so we just need to replace it by the mean of the column.  
```{r}
Fiveyrinfexp['.' == Fiveyrinfexp] = round(mean(as.numeric(Fiveyrinfexp[-which('.' == Fiveyrinfexp)])),2)

be_inflation['.' == be_inflation] = round(mean(as.numeric(be_inflation[-which('.' == be_inflation)])),2)

```


```{r}
final_df = data.frame(dat,btc.xts$Open,btc.xts$High,btc.xts$Low,btc.xts$Close,btc.xts$Adjusted,btc.xts$Volume,rsi,StoFASTK,StoFASTD,StoSLOWK,adx,roc,obv,mfi,ad,MA50,MA200,MACD,MACDSignal,EPU,EMU,Tenyrbond,ThreemTbill,OVX,VIX,Fiveyrinfexp,be_inflation,EMV_IDT)
head(final_df)
```
```{r}
#Execute below code if you want to save final df as csv file.
#write.csv(final_df,"C://Users/manoj/Desktop/BEFD_Project_Images/Final_df.csv")
```

Since our final data frame has lot of NA values we drop for first 50 days of data so now we have data from 07/02/2018(50th point) - 30/10/2022(1282 point)

We also drop MA200 column has 200 NA values in it so i decided to drop the column


```{r}
new_df <- final_df[50:1282,-c(18)]

#new_df <- cbind(row_names = row.names(new_df), new_df)

rownames(new_df) <- c(1:1233)

colnames(new_df)[1] <- "Date"

# Convert Tenyrbond,ThreemTbill,OVX,VIX, Fiveyrinfexp,be_inflation from char to numeric

new_df$Tenyrbond <- as.numeric(new_df$Tenyrbond)
new_df$ThreemTbill <- as.numeric(new_df$ThreemTbill)
new_df$OVX <- as.numeric(new_df$OVX)
new_df$VIX <- as.numeric(new_df$VIX)
new_df$Fiveyrinfexp <- as.numeric(new_df$Fiveyrinfexp)
new_df$be_inflation  <- as.numeric(new_df$be_inflation )

head(new_df)
```
```{r}
#Check which column has NA values in the dataframe

# Load the dplyr package
library(dplyr)

# Count the number of missing values in each column
missing_counts <- colSums(is.na(new_df))

# Print the number of missing values in each column
print(missing_counts)

``` 
```{r}
new_df <- na.omit(new_df)
```


Before starting feature selection there are columns which need to dropped because if high correlation they are Open,High,Low,Close and Volume since we do not know these to known us on day of prediction.
```{r}
library(dplyr)
new_df1 = select(new_df, -c(2,3,4,5,7))

col_names <- names(new_df1)
col_names[2] <- "Price"
col_names[3] <- "RSI"
col_names[9] <- "OBV"
col_names[12] <- "MA_50"
col_names[11] <- "WAD"
col_names[14] <- "MACDSignal"
col_names[8] <- "ROC"
names(new_df1) <- col_names

#new_df1$index <- as.numeric(row.names(new_df1))
#new_df1[order(new_df1$index),]
```
Let's do feature selection using xgboost
```{r}
library(gbm)

boost.model=gbm(Price ~ . ,data=new_df1[,2:22], 
                 distribution="gaussian",n.trees=5000, interaction.depth=8, shrinkage=0.01)

# default vector of parameters
mai.old<-par()$mai
mai.old
#new vector
mai.new<-mai.old
#new space on the left
mai.new[2] <- 2.5 
mai.new
#modify graphical parameters
par(mai=mai.new)

#relative influence plot
summary(boost.model, las=1, cBar=10) 
#cBar defines how many variables
#back to orginal window
par(mai=mai.old)
```
Therefore the modt important WAD (Williams' Accumulation/Distribution), obv(On-balance volume), SMA(50-day moving average), Tenyrbond (Yield on the US ten-year
bond (used for calculating interest rates)),ThreemTbill (three-month T-bill), signal ((MACD Signal) moving average cross-over divergence), OVX(estimate of the expected
volatility over the next thirty days of the US crude oil prices), MFI (Money Flow Index ), be_inflation (break-even inflation), RSI(Relative Strength Index)



Dataset after Top-10 features selected based on importance using Gradient Boosting Models


```{r}
final_df = new_df1[,c('Date','Price','MA_50','OBV','ThreemTbill','RSI','Tenyrbond','MACDSignal','mfi','macd','be_inflation','OVX')]
write.csv(final_df,"C://Users/manoj/Desktop/BEFD_Project_Images/df.csv")
```

ARIMA Modeling

```{r}
plot(new_df1$Price)
```

```{r}
library(stats)

# Convert the data to a time series object
ts_data_train <- ts(final_df$Price[2:1222], frequency = 5)
ts_data_test <- ts(final_df$Price[1222:1232], frequency = 5)
ts_data <- ts(final_df$Price[2:1232], frequency = 5)

# Decompose the time series using the stl function
stl_decomposition <- stl(ts_data_train, s.window = "periodic")

# Extract the seasonality, trend, and remainder components
seasonality <- stl_decomposition$time.series[, 1]
trend <- stl_decomposition$time.series[, 2]
remainder <- stl_decomposition$time.series[, 3]
#acf(new_df1$Price, lag.max=34)
```

The Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series data exhibits stationarity, which is a necessary condition for testing for seasonality. The ADF test returns a p-value and a test statistic, which can be used to assess the likelihood that the data is stationary.

```{r}
library(tseries)
adf.test(ts_data)

```


```{r}
# Plot the seasonality, trend, and remainder components
plot(seasonality, main = "Seasonality")
plot(trend, main = "Trend")
plot(remainder, main = "Remainder")
```

```{r}
#pice <- btc.xts$Adjusted
Lambda<- BoxCox.lambda(ts_data_train)
model = auto.arima(ts_data_train,D=1,approximation = F,allowdrift = T,allowmean = T,lambda = Lambda)
res <- residuals(model)
Acf(res)
```
```{r}
summary(model)
```
```{r}

#pice <- btc.xts$Adjusted

model1 = arima(ts_data_train, order=c(1,0,0))
res1 <- residuals(model1)
Acf(res1)

```
```{r}
summary(model1)
```
```{r}
predictions <- forecast(model1, h=10)
```

Here, ACF plot shows significant autocorrelation at long lag periods, it could indicate that the model is not a good fit for the data and there may be patterns in the residuals that are not being captured by the model. This could suggest that a different model or a different method of fitting the model to the data may be needed

```{r}
fitted(model)

# Set the number of rows and columns of plots

par(mfrow=c(1, 1))

plot(ts_data_train, col="red",type ='b', xlab = 'Weekly Price',ylab="BTC price", pch=16, lty=3,cex=0.6)
mtext("09/02/2018 to 14/10/2022", side=3, line=1)


lines(fitted(model),col="blue")


```
```{r}
predictions <- forecast(model, h=10)
```

```{r}

x <- 1:10
plot(x, ts_data_test[1:10], col="blue",type ='b', xlab = 'Day',ylab="BTC price", pch=16, lty=3,cex=0.6)
mtext("Forecast from 14/10/2022 to 24/10/2022", side=3, line=1)
lines(x, predictions$mean, col='red',xaxt='n')
```
```{r}

AIC(model1)
# Calculate the absolute percentage error for each data point
errors <- abs((predictions$mean - ts_data_test[1:10]) / ts_data_test[1:10])

# Calculate the mean absolute percentage error
mape <- mean(errors) * 100

# Print the result
print(mape)


# Calculate the squared error for each data point
squared_errors <- (predictions$mean - ts_data_test[1:10])^2

# Calculate the mean squared error
mse <- mean(squared_errors)

# Take the square root of the mean squared error
rmse <- sqrt(mse)

# Print the result
print(rmse)
```

```{r}
final_df$Date <- as.Date(final_df$Date, "%d/%m/%Y")
str(final_df)
```
```{r}
# Response variable
summary(final_df$Price)
```
A we see median lies at 10576 and 3rd quartile from the median is much larger than the 1st quartile from the median in a box plot, it could indicate that the data is skewed to the right.

```{r}
boxplot(final_df$Price,col = rgb(0.75, 1, 0.79),main="BTC", ylab="Price")
```
From histogram we can get to know that data is not only right skewed but is right skewed exponentially
```{r}
hist(final_df$Price, col="#04c78d",main="BTC",xlab = "Price", ylab="Frequency")
```

```{r}
#
#explanatory variables
summary(final_df)
```


```{r}
par(mfrow=c(2,2))
for(i in c(3,4,8,9)){
  hist(final_df[,i],col = rgb(0.75, 1, 0.79), main=paste(colnames(final_df)[i]), xlab="")
}
```



```{r}
par(mfrow=c(2,3))
for(i in c(5,6,7,10,11,12)){
  hist(final_df[,i],col = rgb(0.75, 1, 0.79), main=paste(colnames(final_df)[i]), xlab="")
}
```
```{r}
#transform release_date in numeric 
final_df$Date<-as.numeric(final_df$Date)
summary(final_df$Date)



```
```{r}
final_df <- as.data.frame(scale(final_df))
```



```{r}
# Set train and test
set.seed(1)
train = sample (1:nrow(final_df), 0.7*nrow(final_df))
final_df.train=final_df[train ,]
final_df.test=final_df[-train ,]
```

##########################
####Linear Model#########
#########################

```{r}
m1 <- lm(Price~., data=final_df.train)

summary(m1)
```

###############################
###### Stepwise Regression#####

```{r}
m2 <- step(m1, direction="both")
summary(m2)
```
```{r}
#Prediction
p.lm <- predict(m2, newdata=final_df.test)
dev.lm <- sum((p.lm-final_df.test$Price)^2)
err = mean((final_df.test$Price - p.lm)^2)
dev.lm
AIC(m2)
err
```


```{r}
# Calculate the absolute percentage error for each data point
errors <- abs((p.lm - final_df.test$Price) / final_df.test$Price)

# Calculate the mean absolute percentage error
mape <- mean(errors) * 100

# Print the result
print(mape)


# Calculate the squared error for each data point
squared_errors <- (p.lm - final_df.test$Price)^2

# Calculate the mean squared error
mse <- mean(squared_errors)

# Take the square root of the mean squared error
rmse <- sqrt(mse)

# Print the result
print(rmse)

```
The high deviation value indicates that the model is not able to accurately predict the observed values, and the high AIC value indicates that the model is not a good fit to the data

```{r}
library(gam)

###################
##Stepwise GAM
#Start with a linear model (df=1)
g3 <- gam(final_df.train$Price~., data=final_df.train)

#Show the linear effects 
par(mfrow=c(3,5))
plot(g3, se=T) 

#Perform stepwise selection using gam scope
#Values for df should be greater than 1, with df=1 implying a linear fit

sc = gam.scope(final_df.train[,-2], response=2, arg=c("df=2","df=3","df=4"))
g4<- step.Gam(g3, scope=sc, trace=T)
summary(g4)

AIC(g4)
```


```{r}
par(mfrow=c(3,5))
plot(g4, se=T)

# if we want to see better some plot
par(mfrow=c(1,1))
plot(g4, se=T, ask=T)


#Prediction
#data.test[,c(3,7, 10:24)]= lapply(data.test[,c(3,7, 10:24)],factor)

p.gam <- predict(g4,newdata=final_df.test[,-2])     
dev.gam <- sum((p.gam-final_df.test$Price)^2)
dev.gam

```

CART(Classification and Regression Trees)

```{r}
library(tree)

#
set.seed(123)
cb1 <- sample(1:NROW(final_df.train),361)
cb2 <- setdiff(1:NROW(final_df.train),cb1)

#splitting
t1a <- tree(Price~., data=final_df.train[cb1,], control=tree.control(nobs=length(cb1), minsize=2, mindev=0))

#pruning
t1b <- prune.tree(t1a, newdata=final_df.train[cb2,])
#plot(t1b)
#zoom
plot(t1b, xlim=c(0,50))
```

```{r}
J <- t1b$size[which.min(t1b$dev)]

t1c <- prune.tree(t1a, best=J)

#
#Prediction

# make some variables factor
p.tree <- predict(t1c,newdata=final_df.test)     
dev.tree <- sum((p.tree-final_df.test$Price)^2)
err = mean((final_df.test$Price - p.tree)^2)
dev.tree
err
```
For data with preprocessing model fit is relatively poor for CART 

After standard scaling of the data are relatively noise-free and the model is expected to make highly accurate predictions, then a higher deviation value (e.g., 4.605) might be considered poor performance.

################################
###### Gradient Boosting #######
################################

```{r}
library(gbm)
# 1 Boosting- 
boost.model=gbm(Price ~ ., data=final_df.train, 
                 distribution="gaussian", n.trees=5000, interaction.depth=1)
#
#for the plot
par(mfrow=c(1,1))
#
#plot of training error
plot(boost.model$train.error, type="l", ylab="training error")

```

```{r}
# default vector of parameters
mai.old<-par()$mai
mai.old
#new vector
mai.new<-mai.old
#new space on the left
mai.new[2] <- 2.5 
mai.new
#modify graphical parameters
par(mai=mai.new)

#relative influence plot
summary(boost.model, las=1, cBar=10) 
#cBar defines how many variables
#back to orginal window
par(mai=mai.old)
```

```{r}
#test set prediction for every tree (1:5000)


yhat.boost=predict(boost.model, newdata=final_df.test, n.trees=1:5000)

# calculate the error for each iteration
#use 'apply' to perform a 'cycle for' 
# the first element is the matrix we want to use, 2 means 'by column', 
#and the third element indicates the function we want to calculate

err = apply(yhat.boost, 2, function(pred) mean((final_df.test$Price - pred)^2))
#
plot(err, type="l")

```

```{r}
# error comparison (train e test)
plot(boost.model$train.error, type="l")
lines(err, type="l", col=2)
#minimum error in test set
best=which.min(err)
abline(v=best, lty=2, col=4)
#
min(err) #minimum error
```
```{r}
# 2 Boosting- Deeper Trees and small lerning rate
boost.model=gbm(Price ~ ., data=final_df.train, 
                 distribution="gaussian", n.trees=5000, interaction.depth=4, shrinkage=0.01)
#
#for the plot
par(mfrow=c(1,1))
#
#plot of training error
plot(boost.model$train.error, type="l", ylab="training error")
```
```{r}
summary(boost.model, las=1, cBar=10)

yhat.boost=predict(boost.model, newdata=final_df.test, n.trees=1:5000)

err = apply(yhat.boost, 2, function(pred) mean((final_df.test$Price - pred)^2))
#
plot(err, type="l")
```
```{r}
# error comparison (train e test)
plot(boost.model$train.error, type="l")
lines(err, type="l", col=2)
#minimum error in test set
best=which.min(err)
abline(v=best, lty=2, col=4)
#
min(err) #minimum error
```

############################################################################################################### PART2  ######################################## ################################################################

Need to modify the below code and see how it works because new columns are added 
```{r}
library(forecast)
# initial auto-correlation
btc.price <- final_df$Price
Acf(final_df$Price)
pacf(final_df$Price, na.action = na.pass)
```

We cannot observe seasonality in above data since we have Yearly data not monthly or weekly data.

```{r}
# Handling missing values ----
# check for missing values
sum(is.na(btc.price))
```

```{r}
tt <- 1:NROW(final_df$Price)
plot(tt, final_df$Price, xlab="time", ylab="BTC Price", main="Distribution of btc prices over time", col=1, type="b", lty=1, pch=5, cex=0.6)

```
```{r}
# Normal Linear Model ----

# fitting linear model
fit <- lm(btc.price~tt)
summary(fit)
```
According to Rule of Thumb since F-ratio is > 4 we can say our model is significant
```{r}
# plotting regression fit
plot(tt, btc.price, xlab="time", ylab="BTC Price", main="distribution of btc prices over time", col=1, type="b", lty=1, pch=5, cex=0.6)
abline(fit, col=2)
```
```{r}
# Time-Series object Linear Model ----

# transforming data into time-series object
#btc.price.ts <- ts(btc.price, frequency=30) # frequency is 30 due to monthly
ts_data <- ts(final_df$Price[2:1232], frequency = 5)




```
```{r}
# plotting time-series object 
#ts.plot(btc.price.ts, type="l", col="red")
ts.plot(ts_data_train, col="black",xlab= "Week",ylab='Price')

# fitting linear model on time-series object with just trend
fit.t <- tslm(ts_data_train~trend)
summary(fit.t)

```

```{r}
# plotting time-series regression line
ts.plot(ts_data, xlab="Week", ylab="BTC price", main="Plotting time-series regression line of BTC prices over time", col=2)
lines(fitted(fit.t), col=1)
```

```{r}
##check the residuals if they are auto-correlated by using Durbin Watson test
dwtest(fit.t)
```
Since DW result is near to 0 we can say there is +ve autocorrelation.
```{r}
##Finding the residuals
res.trend<- residuals(fit.t)

#Plotting the residuals
plot(res.trend,xlab="Time", ylab="residuals", col=2 )
title('Residual Plot')

```
```{r}
acf(res.trend)
```
```{r}
tslmf= forecast(fit.t, h=10)
```

```{r}
AIC(fit.t)

# Calculate the absolute percentage error for each data point
errors <- abs((tslmf$mean - ts_data_test[1:10]) / ts_data_test[1:10])

# Calculate the mean absolute percentage error
mape <- mean(errors) * 100

# Print the result
print(mape)


# Calculate the squared error for each data point
squared_errors <- (tslmf$mean - ts_data_test[1:10])^2

# Calculate the mean squared error
mse <- mean(squared_errors)

# Take the square root of the mean squared error
rmse <- sqrt(mse)

# Print the result
print(rmse)
```

```{r}
# linear model with both trend and seasonality
fit.ts <- tslm(ts_data~trend+season)
summary(fit.ts)


```
```{r}
ts.plot(btc.price.ts, xlab="time", ylab="BTC price", main="Distribution of BTC prices over time", col=2)
lines(fitted(fit.ts), col=1)

```
```{r}
res.ts <- residuals(fit.ts)
plot(res.ts, col=2)
```
Residuals clearly show a nonlinear behavior there fore we need to use non-linear models.

```{r}
acf(res.ts)
```
We can see from ACF there are many significant auto-correlations between residuals.

```{r}
# Bass Model ----
subset1 <- final_df$Price[700:875]

## visualizing cumulative sum for the bass model
plot(cumsum(subset1), type="b")

plot(subset1, type="b", xlab="Day", ylab="BTC price", pch=16, xaxt="n", lty=3, cex=0.6)
axis(1, at = seq(1,length(subset1),by=35), labels=final_df$Date[seq(700,(699+length(subset1)),by=35)])

```

```{r}
#Bass Model 1----

# Bass Model ----
subset2 <- ts_data_train

## visualizing cumulative sum for the bass model
#plot(cumsum(subset2), type="b")

#plot(subset2, type="b", xlab="Day", ylab="BTC price", pch=16, xaxt="n", lty=3, cex=0.6)
#axis(1, at = seq(1,length(subset1),by=35), labels=final_df$Date[seq(700,(699+length(subset1)),by=35)])
```

```{r}
Acf(subset2)
```

```{r}
# Calculate the absolute percentage error for each data point
errors <- abs((pred_GGM_btc.inst[1222:1231] - ts_data_test[1:10]) / ts_data_test[1:10])

# Calculate the mean absolute percentage error
mape <- mean(errors) * 100

# Print the result
print(mape)


# Calculate the squared error for each data point
squared_errors <- (pred_GGM_btc.inst[1222:1231] - ts_data_test[1:10])^2

# Calculate the mean squared error
mse <- mean(squared_errors)

# Take the square root of the mean squared error
rmse <- sqrt(mse)

# Print the result
print(rmse)
```

```{r}
# applying bass model
bm_prices <- BM(subset2, display=T)
summary(bm_prices)
```


```{r}
# prediction
pred_bmprices <- predict(bm_prices, newx = (1:(10+length(subset2))))
pred.instprices <- make.instantaneous(pred_bmprices)


plot(pred.instprices, col=2, xaxt="n", ylim = c(0,70000),pch=16, lty=3, cex=0.6, xlab="Date", ylab = "Price")
title(expression(paste('Bass Model fitted from 14/10/2020 to 16/06/2021(175 days)','\n',' and forecasted price on next 100 days')), font.main = 2, line = 2, col.main = "blue")
axis(1, at = seq(1,100+length(subset1),by=35), labels=final_df$Date[seq(700,(799+length(subset1)),by=35)])
abline(v = 175, col = "red")
text(175, "Bass Model applied to data up until th red line", col = "red", pos = 4, offset = 0.5)
lines(final_df$Price[700:975], type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, cex=0.6)

```
```{r}
plot(residuals(bm_prices), xlab='No. of Days',ylab ='Residuals',pch=16, lty=3, cex=0.6)
title('Residusal Plot for Bass Model')
```
```{r}



```


```{r}
###we estimate the model with less than 50% of the data

bm_prices50<-BM(subset1[1:135],display = T)
summary(bm_prices50)

pred_bm_prices50<- predict(bm_prices50, newx=c(1:398))
pred.instprices50<- make.instantaneous(pred_bm_prices50)


plot(subset1, type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, cex=0.6)
#, xaxt="n"
#axis(1, at = seq(1,length(subset1),by=20), labels=btc$Date[seq(1001,(1000+length(subset1)),by=20)])
lines(pred.instprices50, lwd=2, col=2)
```
```{r}
###Comparison between models (instantaneous)
###instantaneous
plot(subset1, type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, xaxt="n", cex=0.6)
axis(1, at = seq(1,length(subset1),by=20), labels=btc$Date[seq(1001,(1000+length(subset1)),by=20)])
lines(pred.instprices, lwd=2, col=2)
lines(pred.instprices50, lwd=2, col=3)

```
```{r}
###Comparison between models (cumulative)

plot(cumsum(subset1), type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, xaxt="n", cex=0.6)
axis(1, at = seq(1,length(subset1),by=20), labels=btc$Date[seq(1001,(1000+length(subset1)),by=20)])
lines(pred_bmprices, lwd=2, col=2)
lines(pred_bm_prices50, lwd=2, col=3)
```

```{r}
#(m, p, q are from Bass Model because they can be reasonable starting points)
# rectangular shock (a1 - begin of shock, b1 - end of shock, c1 - intensity of shock(a1 < b1, c1 can be +ve or -ve))
GMBr2btc <- GBM(subset2, shock = "rett", nshock = 3, prelimestimates = c(4.144759e+07, 4.326765e-05, 4.212092e-03,60,75,0.3, 85, 100,-0.3,110, 130,-0.4))
# 
summary(GMBr2btc)

```






```{r}
# cumulative predictions
pred_GMBr2btc<- predict(GMBr2btc, newx = (1:(10+length(subset2))))

# instantaneous predictions
pred_GMBr2btc.inst <- make.instantaneous(pred_GMBr2btc)

plot(pred_GMBr2btc.inst, col=2, xaxt="n", ylim = c(0,70000),pch=16, lty=3, cex=0.6, xlab="Date", ylab = "Price")
title(expression(paste('Generalized Bass Model fitted from 14/10/2020 to 16/06/2021(175 days)','\n',' and forecasted price on next 100 days')), font.main = 2, line = 2, col.main = "blue")
axis(1, at = seq(1,100+length(subset1),by=35), labels=final_df$Date[seq(700,(799+length(subset1)),by=35)])
abline(v= 62, col ='brown')
text(175, "Increased fears of Covid 2nd wave around Jan 9th 2021", col = "brown",adj= 0)
lines(final_df$Price[700:975], type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, cex=0.6)

abline(v = 175, col = "red")
text(175, "GBM fitted to data up until th red line", col = "red",adj=-2)
lines(final_df$Price[700:975], type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, cex=0.6)


```
```{r}
plot(residuals(GMBr2btc), xlab='No. of Days',ylab ='Residuals',pch=16, lty=3, cex=0.6)
title('Residusal Plot for GBM')
```


```{r}
final_df$Date[700+62]
```

Let's see what happened on January 8th,2021 which lead to rise of btc suddenly?

From August 2020 to February 2021 ther is second wave of Covid-19 due to which many has invested their money in notable precious metals like Gold and Bitcoin to during the uncertain times to diversify their portfolio which lead to drastc rise of bitcoin.

```{r}

# residual analysis
res_GMBr2btc <- residuals(GMBr2btc)
plot(res_GMBr2btc, xlab='No. of Residuals', ylab= 'Error')
lines(res_GMBr2btc, col=2, lwd=2)

# seeing auto-correlation of residuals
acf(res_GMBr2btc, plot=T)
```
```{r}
######GGM 

subset1 <- final_df$Price[700:875]
```

```{r}
GGM_btc<- GGM(subset2, prelimestimates=c(4.144759e+07	, 0.001, 0.01, 4.326765e-05, 4.212092e-03))
summary(GGM_btc)
```


```{r}
pred_GGM_btc<- predict(GGM_btc, newx=1:(10+length(subset2)))
pred_GGM_btc.inst<- make.instantaneous(pred_GGM_btc)
```


```{r}
plot(final_df$Price[700:975], type="b", ylim = c(0,70000), xlab="Day", ylab="BTC price", pch=16, lty=3,xaxt ="n", cex=0.6)
axis(1, at = seq(1,length(final_df$Price[700:975]),by=35), 
labels=final_df$Date[seq(700,(799+length(subset1)),by=35)])
abline(v = 175, col = "blue")
text(5, "BM,GBM and GGM fitted to data up until the blue line", col = "blue",adj =-2)
lines(pred_GMBr2btc.inst, lwd=2, col=4)
lines(pred.instprices, lwd=2, col=3)
lines(pred_GGM_btc.inst, lwd=2, col=2)
title(expression(paste('BM,GBM and GGM fitted from 14/10/2020 to 16/06/2021(175 days)','\n',' and forecasted price on next 100 days')), font.main = 1, line = 2, col.main = "blue")

abline(v= 62, col ='brown')
abline(v= 92, col ='brown')

text(175, "Increased fears of Covid 2nd wave around Jan 9 and Feb 19,2021", col = "brown",adj= 0)

# Add a legend to the plot
legend("topright", legend = c("BM","GGM", "GBM"), col = c("red", "green","blue"), lty = 1, lwd = 2)
```
```{r}
###Analysis of residuals
res_GGM_btc<- residuals(GGM_btc)
plot(res_GGM_btc)
lines(res_GGM_btc, col=2, lwd=2)
acf(res_GGM_btc, plot=T)
```


```{r}
### We Perform a competition study between BTC and ETH prices

year<- seq(1,length(btc$Date[1001:1300]))
norm_btc <- btc.price[1001:1300]/max(btc.price[1001:1300])
norm_eth <- eth.price[1001:1300]/max(eth.price[1001:1300])

###make a plot of the two series
plot(year,norm_btc,xlab="year", ylab="Price Distribution",   type= "b", pch=16, lty=3, cex=0.7, col ='black',ylim = c(0,1))
points(year,norm_eth, type= "b", pch=16, lty=3, cex=0.7, col='blue')


```
```{r}

###estimate the UCRCD (with delta and gamma)
ucrcdNR<- UCRCD(norm_btc,norm_eth, display=T)
summary(ucrcdNR)
plot.Dimora(ucrcdNR)


```

```{r}
##we make a plot of the UCRCD model 

plot(year,norm_btc,xlab="year", ylab="Price Distribution",   type= "b", pch=16, lty=3, cex=0.7, col ='black',ylim = c(0,1), sub="The price of ETH grew as a result of investments made with money taken out of BTC market.",cex.lab = 0.1, )
points(year,norm_eth, type= "b", pch=16, lty=3, cex=0.7, col='blue')


lines(year, (ucrcdNR$fitted.i[[1]]), lwd=2, col=1)
lines(year, (ucrcdNR$fitted.i[[2]]), lwd=2, col=2)


###we also add a line and some text within the plot
abline(v=272, lty=2)
#text(50, 0.8, pos=1, "Money withdrawn from BTC is invested in ETH incresed price of it ")

```
Here, the rivalry is advantageous for ETH due to BTC, which increased the market potential in such a way that the normalized ETH had faster growth than BTC at a specific point in time.

```{r}
##Time series analysis: linear regression, ARIMA and ARIMAX models

library(fpp2)
library(forecast)
library(lmtest)
```
```{r}
#Linear regression with trend and seasonality for subset of data

plot(subset1, type="b", xlab="Day", ylab="BTC price", pch=16, lty=3, xaxt="n", cex=0.6)
axis(1, at = seq(1,length(subset1),by=20), labels=btc$Date[seq(1001,(1000+length(subset1)),by=20)])
Acf(subset1)
pacf(subset1)
```



